{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that runs kallisto to index reference file\n",
    "fastq: fastq or fastq.gz file to be indexed for kallisto alignment\n",
    "output: name of desired output file\n",
    "'''\n",
    "def index_transcriptome(fastq, output):\n",
    "    command = \"kallisto index -i \" + output_name + \" \" + file_name\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs the kallisto quantification data on \n",
    "RNAseq_path: path to where all the RNAseq data is kept\n",
    "output_path: path to where the output directories should be stored\n",
    "metadata: file containing on each line a file name, stimulus, and group delimited by a space\n",
    "index: the reference index being used\n",
    "'''\n",
    "def mass_alignments(RNAseq_path, output_path, metadata, index):\n",
    "    # Reads through metadata to get files necessary for alignment\n",
    "    with open(metadata) as f:\n",
    "        for line in f:\n",
    "            \n",
    "            # Parse each line with the space as delimiter\n",
    "            file_stim_group = line.split(\" \")\n",
    "            # Ignores commentted out lines in metadata (i.e. first line)\n",
    "            if line[0] is '>':\n",
    "                continue\n",
    "            \n",
    "            # Make a directory for output files to go\n",
    "            directory = output_path + \"/\" + file_stim_group[1] + \"_\" + file_stim_group[2].strip()\n",
    "            os.system(\"rm -r \" + directory)\n",
    "            os.system(\"mkdir \" + directory)\n",
    "            \n",
    "            # Files of the reads to be align\n",
    "            read1 = RNAseq_path + \"/\" + file_stim_group[0] + \"/trimmomatic_output/\" + \"trimmomatic_output_L001_1P.fastq\"\n",
    "            read2 = RNAseq_path + \"/\" + file_stim_group[0] + \"/trimmomatic_output/\" + \"trimmomatic_output_L001_2P.fastq\"\n",
    "            \n",
    "            # Run kallisto to generate quantification and put into above directory, bootstapping value = 100\n",
    "            command = \"kallisto quant -i \" + index + \" -o \" + directory + \" -b 100 \" + read1 + \" \" + read2\n",
    "            os.system(command)          \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File path of the mouse cDNA dataset\n",
    "mouse_cDNA = \"/Users/jackycheung/Desktop/Coursework/Computational_Genomics/COMS4761_Project/Mus_musculus.GRCm38.cdna.all.fa.gz\"\n",
    "\n",
    "# File paths to be used for alignment\n",
    "RNAseq_path = \"/Volumes/Seagate\\ Backup\\ Plus\\ Drive/PHOSPHOTRAP\\ RNASEQ/K_SMARTBULK_pS6NSTPOOL_RUN001\"\n",
    "output_path = \"/Users/jackycheung/Desktop/Coursework/Computational_Genomics/COMS4761_Project\"\n",
    "metadata = \"/Users/jackycheung/Desktop/Coursework/Computational_Genomics/COMS4761_Project/metadata.txt\"\n",
    "index = \"/Users/jackycheung/Desktop/Coursework/Computational_Genomics/COMS4761_Project/mouse_cDNA.idx\"\n",
    "\n",
    "# Performs the Kallisto indexing of the mouse cDNA dataset and aligns all the RNASeq files\n",
    "# index_transcriptome(mouse_cDNA, mouse_cDNA.idx):\n",
    "# mass_alignments(RNAseq_path, output_path, metadata, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " '''\n",
    "Returns a python dictionary with key = entrez_ID and value = ensembl_ID using conversion file\n",
    "conversion_text: tsv file that contains entrez_ID and ensembl_ID (along with gene description) \n",
    "'''\n",
    "def conversion_dict(conversion_txt):\n",
    "    # Dictionary of conversation to be returned\n",
    "    conversion = {}\n",
    "    \n",
    "    # Parses through the conversation text\n",
    "    with open(conversion_txt) as f:\n",
    "        # Skips the header line in the coversion file\n",
    "        next(f)\n",
    "        \n",
    "        # Breaks up the line and adds conversation to dictionary\n",
    "        for line in f:\n",
    "            columns = line.strip(\"\\n\").split(\"\\t\")\n",
    "            if columns[0] == '':\n",
    "                continue\n",
    "            else:\n",
    "                conversion[columns[0]] = columns[1]\n",
    "    \n",
    "    return conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entrez_to_ensembl = conversion_dict(\"./entrez_ensemble.txt\")\n",
    "ensemble_t_to_g = conversion_dict(\"./ensembl_t_g.txt\")\n",
    "ensemble_g_to_t = conversion_dict(\"./ensembl_g_t.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Returns a dict that represents the metadata of counts and tsv across all CR data\n",
    "metadata_file: file containing the name of the directory containing the abundance.tsv\n",
    "input_path: file containing the directory all the inputs are in\n",
    "'''\n",
    "def phosp_meta(metadata_file, input_path):\n",
    "    # Dictionary for metadata\n",
    "    meta_dict = {}\n",
    "    \n",
    "    num_files = 0\n",
    "    # Parses through the metadata file to generate all CR data name\n",
    "    with open(metadata_file) as f:\n",
    "        for f_line in f:\n",
    "            abundance_file = input_path + \"/\" + f_line.strip() + \"/abundance.tsv\"\n",
    "            num_files += 1\n",
    "            \n",
    "            with open(abundance_file) as a:\n",
    "                next(a)\n",
    "                for a_line in a:\n",
    "                    \n",
    "                    # Parses the counts/tpm out\n",
    "                    columns = a_line.strip().split(\"\\t\")\n",
    "                    ensembl_ID = columns[0]\n",
    "                    counts = float(columns[3])\n",
    "                    tpm = float(columns[4])\n",
    "                    \n",
    "                    # Handles first entry into dictionary\n",
    "                    if ensembl_ID not in meta_dict:\n",
    "                        meta_dict[ensembl_ID] = (counts, tpm)\n",
    "                        \n",
    "                    # Otherwise continue summing (averaging will be done later)\n",
    "                    else:\n",
    "                        meta_dict[ensembl_ID] = (meta_dict[ensembl_ID][0] + counts, meta_dict[ensembl_ID][1] + tpm)\n",
    "        \n",
    "        # Handles averaging of all the data here\n",
    "        for ensembl_ID in meta_dict:\n",
    "            meta_dict[ensembl_ID] = (meta_dict[ensembl_ID][0] / num_files, meta_dict[ensembl_ID][1] / num_files)\n",
    "            \n",
    "    return meta_dict\n",
    "\n",
    "'''\n",
    "Returns a dict that represents the ish data\n",
    "ish_file = file that contains the ish expression data\n",
    "'''\n",
    "def ish_data(ish_file):\n",
    "    \n",
    "    # Dictionary of entrez_id with ISH data (intensity, density, energy)\n",
    "    ish_dict = {}\n",
    "    \n",
    "    # Opens the ISH files to get values for ISH dictionary\n",
    "    with open(ish_file) as f:\n",
    "        # Skips the header file\n",
    "        next(f)\n",
    "        \n",
    "        # Parses line by line and fills the ISH dictionary appropriately\n",
    "        for line in f:\n",
    "            columns = line.strip(\"/n\").split(\",\")\n",
    "            gene_id = columns[11].strip()\n",
    "            # Ignore values with no entrez_id\n",
    "            if gene_id == '':\n",
    "                continue\n",
    "            # Deals with zero expression value by setting intensity to 0\n",
    "            if float(columns[4]) == 0:\n",
    "                expr_intensity = 0\n",
    "            else:\n",
    "                expr_intensity = float(columns[6])/float(columns[4])\n",
    "            expr_density = columns[7]\n",
    "            expr_energy = columns[8]\n",
    "            ish_dict[gene_id] = (expr_intensity, expr_density, expr_energy)\n",
    "            \n",
    "    return ish_dict\n",
    "\n",
    "'''\n",
    "Returns an array of tuple using the ISH Allen Brain Atlas Data and equivalent PhosphoTrap data\n",
    "'''\n",
    "def ish_phos_data(meta_dict, ish_dict, entrez_to_ensembl, ensemble_t_to_g, ensemble_g_to_t):\n",
    "    # Array of tuples containing data in (Ensembl_Gene, Intensity, Density, Energy, Avg_Count, Avg_tphttps://stackoverflow.com/questions/15578331/save-list-of-ordered-tuples-as-csvm) format\n",
    "    ish_phos_meta = []\n",
    "    entrez_ensemble = 0\n",
    "    no_entrez_ensemble = 0\n",
    "    no_equiv_transcript = 0\n",
    "    for entrez_id in ish_dict:\n",
    "        # Checks to see if a conversion exist, skip if does not exist\n",
    "        if entrez_id not in entrez_to_ensembl:\n",
    "            no_entrez_ensemble += 1\n",
    "            continue\n",
    "        # Get equivalenet ensemble gene and transcript version\n",
    "        entrez_ensemble += 1\n",
    "        ensembl_g = entrez_to_ensembl[entrez_id]\n",
    "        ensembl_t = ensemble_g_to_t[ensembl_g]\n",
    "        # Check to see if this entry exist in PhosphoTrap data, skip if does not exist\n",
    "        if ensembl_t not in meta_dict:\n",
    "            no_equiv_transcript += 1\n",
    "            continue\n",
    "        # Puts entry into ish_phos_meta\n",
    "        intensity = ish_dict[entrez_id][0]\n",
    "        density = ish_dict[entrez_id][1]\n",
    "        energy = ish_dict[entrez_id][2]\n",
    "        avg_count = meta_dict[ensembl_t][0]\n",
    "        avg_tpm = meta_dict[ensembl_t][1]\n",
    "        ish_phos_meta.append((ensembl_g, intensity, density, energy, avg_count, avg_tpm))\n",
    "    \n",
    "    print(\"Number of Entrez ID with Ensemble_g Mapping: \", entrez_ensemble)\n",
    "    print(\"Number of Entrez ID with no Ensemble_g Mapping: \", no_entrez_ensemble)\n",
    "    print(\"Number of ISH data with no representation in PhosphoTrap Data: \", no_equiv_transcript)\n",
    "    \n",
    "    return ish_phos_meta\n",
    "\n",
    "'''\n",
    "Converts a array of tuples to a csv files\n",
    "Taken from: https://stackoverflow.com/questions/15578331/save-list-of-ordered-tuples-as-csv\n",
    "'''\n",
    "def convert_to_csv(array_input, output_name):\n",
    "    with open(output_name,'w') as f:\n",
    "        csv_output =csv.writer(f)\n",
    "        csv_output.writerow(['Ensemble_ID','Intensity', 'Density', 'Energy', 'Avg_Count', 'Avg_TPM'])\n",
    "        for row in array_input:\n",
    "            csv_output.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Entrez ID with Ensemble_g Mapping:  17316\n",
      "Number of Entrez ID with no Ensemble_g Mapping:  2102\n",
      "Number of ISH data with no representation in PhosphoTrap Data:  30\n"
     ]
    }
   ],
   "source": [
    "meta_dict = phosp_meta(\"./results/CR_metadata.txt\", \"./results\")\n",
    "ish_dict = ish_data(\"./structure_union_NST.csv\")\n",
    "full_data = ish_phos_data(meta_dict, ish_dict, entrez_to_ensembl, ensemble_t_to_g, ensemble_g_to_t)\n",
    "convert_to_csv(full_data , './output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
